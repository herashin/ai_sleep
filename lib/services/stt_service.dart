// // Whisper API call
// // lib/services/stt_service.dart

// import 'dart:convert';
// import 'dart:io';
// import 'package:flutter_dotenv/flutter_dotenv.dart';
// import 'package:http/http.dart' as http;

// class STTService {
//   /// OpenAI Whisper API í‚¤
//   final String _apiKey = dotenv.env['OPENAI_API_KEY']!;

//   /// ë…¹ìŒëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ Whisper APIë¡œ ì „ì†¡í•˜ì—¬ í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼ ë°˜í™˜
//   Future<String?> transcribeAudio(File audioFile) async {
//     try {
//       final uri = Uri.parse('https://api.openai.com/v1/audio/transcriptions');
//       final request = http.MultipartRequest('POST', uri)
//         ..headers['Authorization'] = 'Bearer $_apiKey'
//         ..fields['model'] = 'whisper-1'
//         ..files.add(
//           await http.MultipartFile.fromPath('file', audioFile.path),
//         );

//       final streamedResponse = await request.send();
//       final response = await http.Response.fromStream(streamedResponse);

//       if (response.statusCode == 200) {
//         final data = json.decode(response.body) as Map<String, dynamic>;
//         return data['text'] as String?;
//       } else {
//         print('STT API ì˜¤ë¥˜: ${response.statusCode}');
//         print('ì‘ë‹µ ë‚´ìš©: ${response.body}');
//         return null;
//       }
//     } catch (e) {
//       print('STT í˜¸ì¶œ ì˜ˆì™¸: \$e');
//       return null;
//     }
//   }
// }

// lib/services/stt_service.dart

import 'dart:async';
import 'package:permission_handler/permission_handler.dart';
import 'package:speech_to_text/speech_to_text.dart';

/// ë„¤ì´í‹°ë¸Œ ìŒì„±ì¸ì‹ì„ ë˜í•‘í•œ STT ì„œë¹„ìŠ¤
class STTService {
  final SpeechToText _speech = SpeechToText();
  bool _initialized = false;

  /// ì´ˆê¸°í™”: ê¶Œí•œ ìš”ì²­ ë° ì—”ì§„ ì´ˆê¸°í™”
  Future<bool> init() async {
    // 1) ë§ˆì´í¬ í¼ë¯¸ì…˜ ìš”ì²­
    final status = await Permission.microphone.request();
    if (!status.isGranted) {
      print('ğŸ”’ ë§ˆì´í¬ ê¶Œí•œ ê±°ë¶€ë¨');
      return false;
    }
    // 2) STT ì—”ì§„ ì´ˆê¸°í™”
    _initialized = await _speech.initialize(
      onStatus: (s) => print('STT status: $s'),
      onError: (e) => print('STT error: $e'),
    );
    return _initialized;
  }

  /// í•œ ë²ˆì˜ ë…¹ìŒ ì„¸ì…˜ìœ¼ë¡œë¶€í„° í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì˜´
  /// _startListening -> ì‚¬ìš©ìê°€ ì¢…ë£Œí•˜ë©´ ìë™ìœ¼ë¡œ resolve
  Future<String?> transcribe() async {
    if (!_initialized && !await init()) return null;

    // ê²°ê³¼ë¥¼ completer ë¡œ ê¸°ë‹¤ë¦¼
    final completer = Completer<String>();

    _speech.listen(
      onResult: (result) {
        // ìµœì¢… ê²°ê³¼ê°€ ë–´ì„ ë•Œë§Œ ë°˜í™˜
        if (result.finalResult) {
          completer.complete(result.recognizedWords);
        }
      },
      localeId: 'ko_KR', // í•„ìš”ì— ë”°ë¼ ë°”ê¾¸ì„¸ìš”
      listenMode: ListenMode.confirmation,
    );

    // ìµœëŒ€ 60ì´ˆ í›„ ìë™ ì¤‘ì§€
    Future.delayed(const Duration(seconds: 60), () {
      if (!completer.isCompleted) {
        _speech.stop();
        completer.completeError('timeout');
      }
    });

    try {
      final text = await completer.future;
      await _speech.stop();
      return text;
    } catch (e) {
      print('STT transcribe ì˜ˆì™¸: $e');
      return null;
    }
  }

  /// (ì˜µì…˜) ì™¸ë¶€ì—ì„œ ê°•ì œ ì¤‘ì§€í•  ë•Œ
  Future<void> stop() => _speech.stop();
}
