import 'dart:convert';
import 'dart:io';
import 'package:flutter/services.dart' show rootBundle;
import 'package:googleapis/speech/v1.dart';
import 'package:googleapis/storage/v1.dart' as gcs;
import 'package:googleapis_auth/auth_io.dart';

class GoogleSTTService {
  final _scopes = [
    SpeechApi.cloudPlatformScope,
    gcs.StorageApi.devstorageFullControlScope,
  ];

  // ğŸ”„ ì „ì²´ íë¦„ì„ ì²˜ë¦¬í•˜ëŠ” í†µí•© í•¨ìˆ˜
  Future<String?> transcribeViaGCS(File audioFile, String bucketName) async {
    try {
      final jsonStr = await rootBundle
          .loadString('assets/mineral-battery-459905-i3-27ecd47986ab.json');
      final creds = ServiceAccountCredentials.fromJson(jsonStr);

      final gcsUri = await _uploadToGCS(audioFile, creds, bucketName);
      return await _transcribeFromGCS(gcsUri, creds);
    } catch (e) {
      print('âŒ ì „ì²´ STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: $e');
      return null;
    }
  }

  // ğŸ“¤ GCS ì—…ë¡œë“œ
  Future<String> _uploadToGCS(
      File file, ServiceAccountCredentials creds, String bucketName) async {
    final client = await clientViaServiceAccount(
        creds, [gcs.StorageApi.devstorageFullControlScope]);
    final storageApi = gcs.StorageApi(client);

    final objectName = file.uri.pathSegments.last;
    final media = gcs.Media(file.openRead(), file.lengthSync());

    await storageApi.objects.insert(
      gcs.Object()..name = objectName,
      bucketName,
      uploadMedia: media,
    );

    client.close();
    print('âœ… GCS ì—…ë¡œë“œ ì™„ë£Œ: gs://$bucketName/$objectName');
    return 'gs://$bucketName/$objectName';
  }

  // ğŸ§  GCS ê¸°ë°˜ STT ìš”ì²­ (LongRunningRecognize)
  Future<String?> _transcribeFromGCS(
      String gcsUri, ServiceAccountCredentials creds) async {
    final client =
        await clientViaServiceAccount(creds, [SpeechApi.cloudPlatformScope]);
    final speechApi = SpeechApi(client);

    final request = LongRunningRecognizeRequest.fromJson({
      'config': {
        'encoding': 'FLAC',
        'sampleRateHertz': 16000,
        'languageCode': 'ko-KR',
        'enableSpeakerDiarization': true,
        'minSpeakerCount': 2,
        'maxSpeakerCount': 3,
        'model': 'latest_long ',
      },
      'audio': {
        'uri': gcsUri,
      },
    });

    final operation = await speechApi.speech.longrunningrecognize(request);
    final opName = operation.name!;

    // ğŸ” ëŒ€ê¸° ë£¨í”„ (í´ë§)
    while (true) {
      final result = await speechApi.operations.get(opName);
      if (result.done == true) {
        final json = result.toJson();

        // response â†’ results â†’ alternatives â†’ transcript
        final responseData = json['response'];
        final results = responseData['results'] as List<dynamic>?;

        if (results != null && results.isNotEmpty) {
          final firstAlternative = results.first['alternatives'][0];

          // ğŸ—£ï¸ í™”ì ë¶„ì„ëœ ë‹¨ì–´ ëª©ë¡ì´ ìˆì„ ê²½ìš°
          final words = firstAlternative['words'] as List<dynamic>?;

          if (words != null && words.isNotEmpty) {
            final buffer = StringBuffer();
            int currentSpeaker = words.first['speakerTag'];
            buffer.write('í™”ì$currentSpeaker: ');

            for (var word in words) {
              final speaker = word['speakerTag'];
              final w = word['word'];
              if (speaker != currentSpeaker) {
                currentSpeaker = speaker;
                buffer.write('\ní™”ì$currentSpeaker: ');
              }
              buffer.write('$w ');
            }

            return buffer.toString().trim();
          } else {
            // í™”ì ì •ë³´ ì—†ìœ¼ë©´ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            return firstAlternative['transcript'];
          }
        }
      }

      await Future.delayed(const Duration(seconds: 2));
    }
  }
}
